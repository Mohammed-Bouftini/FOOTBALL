{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection data Seriea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season: 2016-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACHRAF\\AppData\\Local\\Temp\\ipykernel_5068\\3011069511.py:36: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(html)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Table for 2016-17 to data/raw/seriea\\seriea-2016_17.csv\n"
     ]
    }
   ],
   "source": [
    "#! pip install webdriver_manager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def scrape_table(url, season, headless=True):\n",
    "    driver = setup_driver(headless)\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.NAME, \"season\")))\n",
    "        select = Select(driver.find_element(By.NAME, \"season\"))\n",
    "        select.select_by_value(season)\n",
    "        sleep(5)  # Adjust based on observed page load times\n",
    "\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "        html = driver.page_source\n",
    "        tables = pd.read_html(html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping season {season}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return tables\n",
    "\n",
    "def save_tables(tables, directory, season):\n",
    "    \"\"\"Save each table to a separate CSV file in the specified directory with season included in filename.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    for index, table in enumerate(tables):\n",
    "        file_path = os.path.join(directory, f\"seriea-{season.replace('-', '_')}.csv\")\n",
    "        table.to_csv(file_path, index=False)\n",
    "        print(f\"Saved Table for {season} to {file_path}\")\n",
    "\n",
    "def scrape_years():\n",
    "    url = 'https://www.legaseriea.it/en/serie-a/classifica'\n",
    "    seasons = [f\"{year}-{str(year+1)[-2:]}\" for year in range(1986, 2024)]\n",
    "    for season in seasons:\n",
    "        print(f\"Scraping data for season: {season}\")\n",
    "        tables = scrape_table(url, season, headless=False)\n",
    "        if tables:\n",
    "            save_tables(tables, \"data/raw/seriea\", season)\n",
    "        else:\n",
    "            print(\"No tables found for season:\", season)\n",
    "\n",
    "scrape_years()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection data bundesliga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season: 1963-1964\n",
      "Saved Table for 1963-1964 to data/raw/bundesliga\\bundesliga-1963_1964.csv\n",
      "Scraping data for season: 1964-1965\n",
      "Saved Table for 1964-1965 to data/raw/bundesliga\\bundesliga-1964_1965.csv\n",
      "Scraping data for season: 1965-1966\n",
      "Saved Table for 1965-1966 to data/raw/bundesliga\\bundesliga-1965_1966.csv\n",
      "Scraping data for season: 1966-1967\n",
      "Saved Table for 1966-1967 to data/raw/bundesliga\\bundesliga-1966_1967.csv\n",
      "Scraping data for season: 1967-1968\n",
      "Saved Table for 1967-1968 to data/raw/bundesliga\\bundesliga-1967_1968.csv\n",
      "Scraping data for season: 1968-1969\n",
      "Saved Table for 1968-1969 to data/raw/bundesliga\\bundesliga-1968_1969.csv\n",
      "Scraping data for season: 1969-1970\n",
      "Saved Table for 1969-1970 to data/raw/bundesliga\\bundesliga-1969_1970.csv\n",
      "Scraping data for season: 1970-1971\n",
      "Saved Table for 1970-1971 to data/raw/bundesliga\\bundesliga-1970_1971.csv\n",
      "Scraping data for season: 1971-1972\n",
      "Saved Table for 1971-1972 to data/raw/bundesliga\\bundesliga-1971_1972.csv\n",
      "Scraping data for season: 1972-1973\n",
      "Saved Table for 1972-1973 to data/raw/bundesliga\\bundesliga-1972_1973.csv\n",
      "Scraping data for season: 1973-1974\n",
      "Saved Table for 1973-1974 to data/raw/bundesliga\\bundesliga-1973_1974.csv\n",
      "Scraping data for season: 1974-1975\n",
      "Saved Table for 1974-1975 to data/raw/bundesliga\\bundesliga-1974_1975.csv\n",
      "Scraping data for season: 1975-1976\n",
      "Saved Table for 1975-1976 to data/raw/bundesliga\\bundesliga-1975_1976.csv\n",
      "Scraping data for season: 1976-1977\n",
      "Saved Table for 1976-1977 to data/raw/bundesliga\\bundesliga-1976_1977.csv\n",
      "Scraping data for season: 1977-1978\n",
      "Saved Table for 1977-1978 to data/raw/bundesliga\\bundesliga-1977_1978.csv\n",
      "Scraping data for season: 1978-1979\n",
      "Saved Table for 1978-1979 to data/raw/bundesliga\\bundesliga-1978_1979.csv\n",
      "Scraping data for season: 1979-1980\n",
      "Saved Table for 1979-1980 to data/raw/bundesliga\\bundesliga-1979_1980.csv\n",
      "Scraping data for season: 1980-1981\n",
      "Saved Table for 1980-1981 to data/raw/bundesliga\\bundesliga-1980_1981.csv\n",
      "Scraping data for season: 1981-1982\n",
      "Saved Table for 1981-1982 to data/raw/bundesliga\\bundesliga-1981_1982.csv\n",
      "Scraping data for season: 1982-1983\n",
      "Saved Table for 1982-1983 to data/raw/bundesliga\\bundesliga-1982_1983.csv\n",
      "Scraping data for season: 1983-1984\n",
      "Saved Table for 1983-1984 to data/raw/bundesliga\\bundesliga-1983_1984.csv\n",
      "Scraping data for season: 1984-1985\n",
      "Saved Table for 1984-1985 to data/raw/bundesliga\\bundesliga-1984_1985.csv\n",
      "Scraping data for season: 1985-1986\n",
      "Saved Table for 1985-1986 to data/raw/bundesliga\\bundesliga-1985_1986.csv\n",
      "Scraping data for season: 1986-1987\n",
      "Saved Table for 1986-1987 to data/raw/bundesliga\\bundesliga-1986_1987.csv\n",
      "Scraping data for season: 1987-1988\n",
      "Saved Table for 1987-1988 to data/raw/bundesliga\\bundesliga-1987_1988.csv\n",
      "Scraping data for season: 1988-1989\n",
      "Saved Table for 1988-1989 to data/raw/bundesliga\\bundesliga-1988_1989.csv\n",
      "Scraping data for season: 1989-1990\n",
      "Saved Table for 1989-1990 to data/raw/bundesliga\\bundesliga-1989_1990.csv\n",
      "Scraping data for season: 1990-1991\n",
      "Saved Table for 1990-1991 to data/raw/bundesliga\\bundesliga-1990_1991.csv\n",
      "Scraping data for season: 1991-1992\n",
      "Saved Table for 1991-1992 to data/raw/bundesliga\\bundesliga-1991_1992.csv\n",
      "Scraping data for season: 1992-1993\n",
      "Saved Table for 1992-1993 to data/raw/bundesliga\\bundesliga-1992_1993.csv\n",
      "Scraping data for season: 1993-1994\n",
      "Saved Table for 1993-1994 to data/raw/bundesliga\\bundesliga-1993_1994.csv\n",
      "Scraping data for season: 1994-1995\n",
      "Saved Table for 1994-1995 to data/raw/bundesliga\\bundesliga-1994_1995.csv\n",
      "Scraping data for season: 1995-1996\n",
      "Saved Table for 1995-1996 to data/raw/bundesliga\\bundesliga-1995_1996.csv\n",
      "Scraping data for season: 1996-1997\n",
      "Saved Table for 1996-1997 to data/raw/bundesliga\\bundesliga-1996_1997.csv\n",
      "Scraping data for season: 1997-1998\n",
      "Saved Table for 1997-1998 to data/raw/bundesliga\\bundesliga-1997_1998.csv\n",
      "Scraping data for season: 1998-1999\n",
      "Saved Table for 1998-1999 to data/raw/bundesliga\\bundesliga-1998_1999.csv\n",
      "Scraping data for season: 1999-2000\n",
      "Saved Table for 1999-2000 to data/raw/bundesliga\\bundesliga-1999_2000.csv\n",
      "Scraping data for season: 2000-2001\n",
      "Saved Table for 2000-2001 to data/raw/bundesliga\\bundesliga-2000_2001.csv\n",
      "Scraping data for season: 2001-2002\n",
      "Saved Table for 2001-2002 to data/raw/bundesliga\\bundesliga-2001_2002.csv\n",
      "Scraping data for season: 2002-2003\n",
      "Saved Table for 2002-2003 to data/raw/bundesliga\\bundesliga-2002_2003.csv\n",
      "Scraping data for season: 2003-2004\n",
      "Saved Table for 2003-2004 to data/raw/bundesliga\\bundesliga-2003_2004.csv\n",
      "Scraping data for season: 2004-2005\n",
      "Saved Table for 2004-2005 to data/raw/bundesliga\\bundesliga-2004_2005.csv\n",
      "Scraping data for season: 2005-2006\n",
      "Saved Table for 2005-2006 to data/raw/bundesliga\\bundesliga-2005_2006.csv\n",
      "Scraping data for season: 2006-2007\n",
      "Saved Table for 2006-2007 to data/raw/bundesliga\\bundesliga-2006_2007.csv\n",
      "Scraping data for season: 2007-2008\n",
      "Saved Table for 2007-2008 to data/raw/bundesliga\\bundesliga-2007_2008.csv\n",
      "Scraping data for season: 2008-2009\n",
      "Saved Table for 2008-2009 to data/raw/bundesliga\\bundesliga-2008_2009.csv\n",
      "Scraping data for season: 2009-2010\n",
      "Saved Table for 2009-2010 to data/raw/bundesliga\\bundesliga-2009_2010.csv\n",
      "Scraping data for season: 2010-2011\n",
      "Saved Table for 2010-2011 to data/raw/bundesliga\\bundesliga-2010_2011.csv\n",
      "Scraping data for season: 2011-2012\n",
      "Saved Table for 2011-2012 to data/raw/bundesliga\\bundesliga-2011_2012.csv\n",
      "Scraping data for season: 2012-2013\n",
      "Saved Table for 2012-2013 to data/raw/bundesliga\\bundesliga-2012_2013.csv\n",
      "Scraping data for season: 2013-2014\n",
      "Saved Table for 2013-2014 to data/raw/bundesliga\\bundesliga-2013_2014.csv\n",
      "Scraping data for season: 2014-2015\n",
      "Saved Table for 2014-2015 to data/raw/bundesliga\\bundesliga-2014_2015.csv\n",
      "Scraping data for season: 2015-2016\n",
      "Saved Table for 2015-2016 to data/raw/bundesliga\\bundesliga-2015_2016.csv\n",
      "Scraping data for season: 2016-2017\n",
      "Saved Table for 2016-2017 to data/raw/bundesliga\\bundesliga-2016_2017.csv\n",
      "Scraping data for season: 2017-2018\n",
      "Saved Table for 2017-2018 to data/raw/bundesliga\\bundesliga-2017_2018.csv\n",
      "Scraping data for season: 2018-2019\n",
      "Saved Table for 2018-2019 to data/raw/bundesliga\\bundesliga-2018_2019.csv\n",
      "Scraping data for season: 2019-2020\n",
      "Saved Table for 2019-2020 to data/raw/bundesliga\\bundesliga-2019_2020.csv\n",
      "Scraping data for season: 2020-2021\n",
      "Saved Table for 2020-2021 to data/raw/bundesliga\\bundesliga-2020_2021.csv\n",
      "Scraping data for season: 2021-2022\n",
      "Saved Table for 2021-2022 to data/raw/bundesliga\\bundesliga-2021_2022.csv\n",
      "Scraping data for season: 2022-2023\n",
      "Saved Table for 2022-2023 to data/raw/bundesliga\\bundesliga-2022_2023.csv\n",
      "Scraping data for season: 2023-2024\n",
      "Saved Table for 2023-2024 to data/raw/bundesliga\\bundesliga-2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    \"\"\"Set up Chrome WebDriver with options.\"\"\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    options.add_argument(\"--disable-javascript\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_page_load_timeout(90)\n",
    "    return driver\n",
    "\n",
    "def scrape_table(url, season, headless=True):\n",
    "    \"\"\"Scrapes the table for the given season.\"\"\"\n",
    "    driver = setup_driver(headless)\n",
    "    attempts = 0\n",
    "    max_attempts = 3\n",
    "    season_url = url.format(season=season.replace('/', '-'))  # Ensure URL format is correct\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            driver.get(season_url)\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "            html = driver.page_source\n",
    "            tables = pd.read_html(StringIO(html))\n",
    "            return tables\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1}: Error during scraping {season}: {e}\")\n",
    "            attempts += 1\n",
    "            driver.quit()\n",
    "            driver = setup_driver(headless)\n",
    "        finally:\n",
    "            if attempts == max_attempts:\n",
    "                driver.quit()\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_tables(tables, directory, season):\n",
    "    \"\"\"Saves the scraped tables to a CSV file.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    file_path = os.path.join(directory, f\"bundesliga-{season.replace('-', '_')}.csv\")\n",
    "    tables[0].to_csv(file_path, index=False)\n",
    "    print(f\"Saved Table for {season} to {file_path}\")\n",
    "\n",
    "def scrape_years():\n",
    "    \"\"\"Initiates scraping for a range of seasons.\"\"\"\n",
    "    base_url = 'https://www.bundesliga.com/en/bundesliga/table/{season}'\n",
    "    seasons = [f\"{year}-{year+1}\" for year in range(1963, 2024)]\n",
    "    for season in seasons:\n",
    "        print(f\"Scraping data for season: {season}\")\n",
    "        tables = scrape_table(base_url, season, headless=False)\n",
    "        if tables:\n",
    "            save_tables(tables, \"data/raw/bundesliga\", season)\n",
    "        else:\n",
    "            print(f\"No tables found for {season}.\")\n",
    "\n",
    "scrape_years()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection data ligue1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for season 1993-1994...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_1993-1994.csv\n",
      "Downloading data for season 1994-1995...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_1994-1995.csv\n",
      "Downloading data for season 1995-1996...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_1995-1996.csv\n",
      "Downloading data for season 1996-1997...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_1996-1997.csv\n",
      "Downloading data for season 1997-1998...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_1997-1998.csv\n",
      "Downloading data for season 1998-1999...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_1998-1999.csv\n",
      "Downloading data for season 1999-2000...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_1999-2000.csv\n",
      "Downloading data for season 2000-2001...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2000-2001.csv\n",
      "Downloading data for season 2001-2002...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2001-2002.csv\n",
      "Downloading data for season 2002-2003...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2002-2003.csv\n",
      "Downloading data for season 2003-2004...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2003-2004.csv\n",
      "Downloading data for season 2004-2005...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2004-2005.csv\n",
      "Downloading data for season 2005-2006...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2005-2006.csv\n",
      "Downloading data for season 2006-2007...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2006-2007.csv\n",
      "Downloading data for season 2007-2008...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2007-2008.csv\n",
      "Downloading data for season 2008-2009...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2008-2009.csv\n",
      "Downloading data for season 2009-2010...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2009-2010.csv\n",
      "Downloading data for season 2010-2011...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2010-2011.csv\n",
      "Downloading data for season 2011-2012...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2011-2012.csv\n",
      "Downloading data for season 2012-2013...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2012-2013.csv\n",
      "Downloading data for season 2013-2014...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2013-2014.csv\n",
      "Downloading data for season 2014-2015...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2014-2015.csv\n",
      "Downloading data for season 2015-2016...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2015-2016.csv\n",
      "Downloading data for season 2016-2017...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2016-2017.csv\n",
      "Downloading data for season 2017-2018...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2017-2018.csv\n",
      "Downloading data for season 2018-2019...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2018-2019.csv\n",
      "Downloading data for season 2019-2020...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2019-2020.csv\n",
      "Downloading data for season 2020-2021...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2020-2021.csv\n",
      "Downloading data for season 2021-2022...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2021-2022.csv\n",
      "Downloading data for season 2022-2023...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2022-2023.csv\n",
      "Downloading data for season 2023-2024...\n",
      "Successfully downloaded and saved data to data/raw/ligue1/ligue1_2023-2024.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up and return a Selenium WebDriver.\"\"\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode.\n",
    "    options.add_argument('--no-sandbox')  # Bypass OS security model, required for Docker.\n",
    "    options.add_argument('--disable-dev-shm-usage')  # Overcome limited resource problems.\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def download_data_with_selenium(url, xpath):\n",
    "    \"\"\"Download data from the specified URL using Selenium and the specified XPath.\"\"\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    try:\n",
    "        # Wait until the elements from the XPath are loaded\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n",
    "        \n",
    "        # Collect data\n",
    "        data = []\n",
    "        elements = driver.find_elements(By.XPATH, xpath)\n",
    "        for element in elements:\n",
    "            data.append(element.text.split('\\n'))\n",
    "\n",
    "        # Create DataFrame\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def save_data(df, filename):\n",
    "    \"\"\"Save the DataFrame to CSV.\"\"\"\n",
    "    if not df.empty:\n",
    "        file_path = f'data/raw/ligue1/{filename}.csv'\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Successfully downloaded and saved data to {file_path}')\n",
    "    else:\n",
    "        print(f'No data to save for {filename}.')\n",
    "\n",
    "def main():\n",
    "    base_url = 'https://www.ligue1.com/ranking?seasonId='\n",
    "    seasons = [f\"{year}-{year+1}\" for year in range(1993, 2024)]  # Adjust range as needed\n",
    "    xpath = '//a[contains(@class, \"GeneralStats-link\")]'\n",
    "\n",
    "    for season in seasons:\n",
    "        url = f\"{base_url}{season}&StatsActiveTab=0\"\n",
    "        filename = f'ligue1_{season}'\n",
    "        try:\n",
    "            print(f\"Downloading data for season {season}...\")\n",
    "            df = download_data_with_selenium(url, xpath)\n",
    "            save_data(df, filename)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to download data for season {season}: {e}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection laliga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season: 2003-2004\n",
      "Saved Table for 2003-2004 to data/raw/laliga\\laliga-2003_2004.csv\n",
      "Scraping data for season: 2004-2005\n",
      "Saved Table for 2004-2005 to data/raw/laliga\\laliga-2004_2005.csv\n",
      "Scraping data for season: 2005-2006\n",
      "Saved Table for 2005-2006 to data/raw/laliga\\laliga-2005_2006.csv\n",
      "Scraping data for season: 2006-2007\n",
      "Saved Table for 2006-2007 to data/raw/laliga\\laliga-2006_2007.csv\n",
      "Scraping data for season: 2007-2008\n",
      "Saved Table for 2007-2008 to data/raw/laliga\\laliga-2007_2008.csv\n",
      "Scraping data for season: 2008-2009\n",
      "Saved Table for 2008-2009 to data/raw/laliga\\laliga-2008_2009.csv\n",
      "Scraping data for season: 2009-2010\n",
      "Saved Table for 2009-2010 to data/raw/laliga\\laliga-2009_2010.csv\n",
      "Scraping data for season: 2010-2011\n",
      "Saved Table for 2010-2011 to data/raw/laliga\\laliga-2010_2011.csv\n",
      "Scraping data for season: 2011-2012\n",
      "Saved Table for 2011-2012 to data/raw/laliga\\laliga-2011_2012.csv\n",
      "Scraping data for season: 2012-2013\n",
      "Saved Table for 2012-2013 to data/raw/laliga\\laliga-2012_2013.csv\n",
      "Scraping data for season: 2013-2014\n",
      "Saved Table for 2013-2014 to data/raw/laliga\\laliga-2013_2014.csv\n",
      "Scraping data for season: 2014-2015\n",
      "Saved Table for 2014-2015 to data/raw/laliga\\laliga-2014_2015.csv\n",
      "Scraping data for season: 2015-2016\n",
      "Saved Table for 2015-2016 to data/raw/laliga\\laliga-2015_2016.csv\n",
      "Scraping data for season: 2016-2017\n",
      "Saved Table for 2016-2017 to data/raw/laliga\\laliga-2016_2017.csv\n",
      "Scraping data for season: 2017-2018\n",
      "Saved Table for 2017-2018 to data/raw/laliga\\laliga-2017_2018.csv\n",
      "Scraping data for season: 2018-2019\n",
      "Saved Table for 2018-2019 to data/raw/laliga\\laliga-2018_2019.csv\n",
      "Scraping data for season: 2019-2020\n",
      "Saved Table for 2019-2020 to data/raw/laliga\\laliga-2019_2020.csv\n",
      "Scraping data for season: 2020-2021\n",
      "Saved Table for 2020-2021 to data/raw/laliga\\laliga-2020_2021.csv\n",
      "Scraping data for season: 2021-2022\n",
      "Saved Table for 2021-2022 to data/raw/laliga\\laliga-2021_2022.csv\n",
      "Scraping data for season: 2022-2023\n",
      "Saved Table for 2022-2023 to data/raw/laliga\\laliga-2022_2023.csv\n",
      "Scraping data for season: 2023-2024\n",
      "Saved Table for 2023-2024 to data/raw/laliga\\laliga-2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    \"\"\"Set up Chrome WebDriver with options.\"\"\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    options.add_argument(\"--disable-javascript\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_page_load_timeout(90)\n",
    "    return driver\n",
    "\n",
    "def scrape_table(url, season, headless=True):\n",
    "    \"\"\"Scrapes the table for the given season.\"\"\"\n",
    "    driver = setup_driver(headless)\n",
    "    attempts = 0\n",
    "    max_attempts = 3\n",
    "    season_url = url.format(season=season.replace('/', '-'))  # Ensure URL format is correct\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            driver.get(season_url)\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "            html = driver.page_source\n",
    "            tables = pd.read_html(StringIO(html))\n",
    "            return tables\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1}: Error during scraping {season}: {e}\")\n",
    "            attempts += 1\n",
    "            driver.quit()\n",
    "            driver = setup_driver(headless)\n",
    "        finally:\n",
    "            if attempts == max_attempts:\n",
    "                driver.quit()\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_tables(tables, directory, season):\n",
    "    \"\"\"Saves the scraped tables to a CSV file.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    file_path = os.path.join(directory, f\"laliga-{season.replace('-', '_')}.csv\")\n",
    "    tables[0].to_csv(file_path, index=False)\n",
    "    print(f\"Saved Table for {season} to {file_path}\")\n",
    "\n",
    "def scrape_years():\n",
    "    \"\"\"Initiates scraping for a range of seasons.\"\"\"\n",
    "    base_url = 'https://www.footmercato.net/espagne/liga/{season}/classement'\n",
    "    seasons = [f\"{year}-{year+1}\" for year in range(2003, 2024)]\n",
    "    for season in seasons:\n",
    "        print(f\"Scraping data for season: {season}\")\n",
    "        tables = scrape_table(base_url, season, headless=False)\n",
    "        if tables:\n",
    "            save_tables(tables, \"data/raw/laliga\", season)\n",
    "        else:\n",
    "            print(f\"No tables found for {season}.\")\n",
    "\n",
    "scrape_years()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection premierleague\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season: 1998-99\n",
      "File path: data/raw/premierleague\\premierleague-1998_99.csv\n",
      "Saved Table for 1998-99 to data/raw/premierleague\\premierleague-1998_99.csv\n",
      "Scraping data for season: 2006-07\n",
      "File path: data/raw/premierleague\\premierleague-2006_07.csv\n",
      "Saved Table for 2006-07 to data/raw/premierleague\\premierleague-2006_07.csv\n",
      "Scraping data for season: 2007-08\n",
      "File path: data/raw/premierleague\\premierleague-2007_08.csv\n",
      "Saved Table for 2007-08 to data/raw/premierleague\\premierleague-2007_08.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Setup environment for WebDriver\n",
    "os.environ['WDM_LOCAL'] = '1'\n",
    "os.environ['WDM_LOG_LEVEL'] = '0'\n",
    "os.environ['WDM_CACHE_DIR'] = os.path.join(os.path.expanduser('~'), '.wdm')\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    manager = ChromeDriverManager()\n",
    "    service = Service(manager.install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--verbose')\n",
    "    options.add_argument('--log-path=chromedriver.log')\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def dismiss_ads(driver):\n",
    "    try:\n",
    "        # Execute JavaScript to close ads\n",
    "        driver.execute_script(\"\"\"\n",
    "            var closeButton = document.getElementById('advertClose');\n",
    "            if (closeButton) {\n",
    "                closeButton.click();\n",
    "            }\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(\"Error dismissing ads: \", str(e))\n",
    "\n",
    "\n",
    "def filter_by_season(driver, season_id):\n",
    "    try:\n",
    "        # Wait until the season dropdown is present\n",
    "        season_dropdown = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".current[data-dropdown-current='compSeasons']\"))\n",
    "        )\n",
    "        \n",
    "        # Click on the season dropdown\n",
    "        season_dropdown.click()\n",
    "        time.sleep(5)\n",
    "        # Find the dropdown options\n",
    "        dropdown_options = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul[data-dropdown-list='compSeasons'] li\"))\n",
    "        )\n",
    "        \n",
    "        # Loop through the dropdown options\n",
    "        for option in dropdown_options:\n",
    "            # Check if the option's data-option-id matches the desired season_id\n",
    "            if option.get_attribute(\"data-option-id\") == season_id:\n",
    "                # Click on the option\n",
    "                option.click()\n",
    "                break\n",
    "\n",
    "         # Add a small delay before proceeding\n",
    "        time.sleep(5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error clicking season element: \", str(e))\n",
    "\n",
    "def scrape_table(url, season_id, headless=False):\n",
    "    driver = setup_driver(headless)\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        dismiss_ads(driver)\n",
    "        \n",
    "        # Click on the dropdown to select the season\n",
    "        filter_by_season(driver, season_id)  # Correction ici\n",
    "        \n",
    "        # Wait until the table is present\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "        )\n",
    "        \n",
    "        # Read the HTML content and parse it using pandas\n",
    "        html = driver.page_source\n",
    "        html_io = StringIO(html)\n",
    "        tables = pd.read_html(html_io)\n",
    "        \n",
    "        if not tables:\n",
    "            raise ValueError(\"No tables found on the page\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping season {season_id}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return tables\n",
    "\n",
    "\n",
    "def save_tables(tables, directory, season):\n",
    "    if tables:\n",
    "        os.makedirs(directory, exist_ok=True)  # Crée le répertoire s'il n'existe pas\n",
    "        file_path = os.path.join(directory, f\"premierleague-{season.replace('-', '_')}.csv\")\n",
    "        \n",
    "        # Afficher le chemin pour le débogage\n",
    "        print(\"File path:\", file_path)\n",
    "        \n",
    "        tables[0].to_csv(file_path, index=False)  # Enregistre la première table\n",
    "        print(f\"Saved Table for {season} to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def scrape_years():\n",
    "    url = 'https://www.premierleague.com/tables'\n",
    "    season_ids = {\n",
    "        \"1992-93\": \"1\" , \"1993-94\": \"2\",\"1994-95\": \"3\",\"1995-96\": \"4\", \"1996-97\": \"5\",\"1997-98\": \"6\",\"1998-99\": \"7\",\"1999-00\": \"8\",   \n",
    "        \"2000-01\": \"9\",\"2001-02\": \"10\",\"2002-03\": \"11\",\"2003-04\": \"12\",\"2004-05\": \"13\",\"2005-06\": \"14\",\"2006-07\": \"15\",\"2007-08\": \"16\",\n",
    "        \"2008-09\": \"17\",\"2009-10\": \"18\",\"2010-11\": \"19\",\"2011-12\": \"20\", \"2012-13\": \"21\",\"2013-14\": \"22\",\"2014-15\": \"27\",\"2015-16\": \"42\",\n",
    "        \"2016-17\": \"54\",\"2017-18\": \"79\",\"2018-19\": \"210\",\"2019-20\": \"274\",\"2020-21\": \"363\",\"2021-22\": \"418\",\"2022-23\": \"489\",\"2023-24\": \"578\"\n",
    "    \n",
    "    }\n",
    "   \n",
    "    for season, id in season_ids.items():\n",
    "        print(f\"Scraping data for season: {season}\")\n",
    "        tables = scrape_table(url, id, headless=False)\n",
    "        if tables:\n",
    "            save_tables(tables, \"data/raw/premierleague\", season)\n",
    "        else:\n",
    "            print(f\"No tables found for season: {season}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrape_years()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
